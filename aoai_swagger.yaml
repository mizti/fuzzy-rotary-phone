---
swagger: "2.0"
info:
  title: Azure OpenAI API
  version: "1.0"
  description: Azure OpenAI APIs for completions and search
host: xxxx.openai.azure.com
basePath: /openai
schemes:
  - https
securityDefinitions: {}
security: []
paths:
  "/deployments/{deployment-id}/chat/completions":
    post:
      description: Creates a completion for the chat message
      operationId: Chat
      summary: Creates a completion for the chat message
      parameters:
        - name: deployment-id
          in: path
          required: true
          type: string
        - name: api-version
          in: query
          required: true
          type: string
          default: 2023-05-15
        - name: api-key
          in: header
          required: true
          type: string
          format: password
        - name: ChatCompletionsPostRequest
          in: body
          schema:
            required:
              - messages
            type: object
            properties:
              messages:
                description: The messages to generate chat completions for, in the chat format.
                minItems: 1
                type: array
                items:
                  required:
                    - role
                    - content
                  type: object
                  properties:
                    role:
                      description: The role of the author of this message.
                      enum:
                        - system
                        - user
                        - assistant
                      type: string
                    content:
                      description: The contents of the message
                      type: string
                    name:
                      description: The name of the user in a multi-user chat
                      type: string
                      title: ""
              temperature:
                description: >-
                  What sampling temperature to use, between 0 and 2. Higher values
                  like 0.8 will make the output more random, while lower values
                  like 0.2 will make it more focused and deterministic.

                  We generally recommend altering this or `top_p` but not both.
                default: 1
                maximum: 2
                minimum: 0
                type: number
                example: 1
              top_p:
                description: >-
                  An alternative to sampling with temperature, called nucleus
                  sampling, where the model considers the results of the tokens
                  with top_p probability mass. So 0.1 means only the tokens
                  comprising the top 10% probability mass are considered.

                  We generally recommend altering this or `temperature` but not both.
                default: 1
                maximum: 1
                minimum: 0
                type: number
                example: 1
              n:
                description: How many chat completion choices to generate for each input
                  message.
                default: 1
                maximum: 128
                minimum: 1
                type: integer
                example: 1
              stream:
                description: "If set, partial message deltas will be sent, like in ChatGPT.
                  Tokens will be sent as data-only server-sent events as they
                  become available, with the stream terminated by a `data:
                  [DONE]` message."
                type: boolean
                title: ""
                enum:
                  - ""
                  - true
                  - false
              stop:
                description: Up to 4 sequences where the API will stop generating further
                  tokens.
                default: null
                allOf:
                  - type: string
              max_tokens:
                description: The maximum number of tokens allowed for the generated answer. By
                  default, the number of tokens the model can return will be
                  (4096 - prompt tokens).
                type: integer
                title: ""
                format: int32
              presence_penalty:
                description: Number between -2.0 and 2.0. Positive values penalize new tokens
                  based on whether they appear in the text so far, increasing
                  the model's likelihood to talk about new topics.
                default: 0
                maximum: 2
                minimum: -2
                type: number
              frequency_penalty:
                description: Number between -2.0 and 2.0. Positive values penalize new tokens
                  based on their existing frequency in the text so far,
                  decreasing the model's likelihood to repeat the same line
                  verbatim.
                maximum: 2
                minimum: -2
                type: number
                title: ""
                format: float
              logit_bias:
                description: Modify the likelihood of specified tokens appearing in the
                  completion. Accepts a json object that maps tokens (specified
                  by their token ID in the tokenizer) to an associated bias
                  value from -100 to 100. Mathematically, the bias is added to
                  the logits generated by the model prior to sampling. The exact
                  effect will vary per model, but values between -1 and 1 should
                  decrease or increase likelihood of selection; values like -100
                  or 100 should result in a ban or exclusive selection of the
                  relevant token.
                type: object
              user:
                description: A unique identifier representing your end-user, which can help
                  Azure OpenAI to monitor and detect abuse.
                type: string
                example: user-1234
            example:
              model: gpt-35-turbo
              messages:
                - role: user
                  content: Hello!
      consumes:
        - application/json
      produces:
        - application/json
      responses:
        "200":
          description: OK
          schema:
            required:
              - id
              - object
              - created
              - model
              - choices
            type: object
            properties:
              id:
                type: string
              object:
                type: string
              created:
                format: int32
                type: integer
                title: ""
              model:
                type: string
              choices:
                type: array
                items:
                  type: object
                  properties:
                    index:
                      type: integer
                      title: ""
                      format: int32
                    message:
                      required:
                        - role
                        - content
                      type: object
                      properties:
                        role:
                          description: The role of the author of this message.
                          enum:
                            - system
                            - user
                            - assistant
                          type: string
                        content:
                          description: The contents of the message
                          type: string
                    finish_reason:
                      type: string
              usage:
                required:
                  - prompt_tokens
                  - completion_tokens
                  - total_tokens
                type: object
                properties:
                  prompt_tokens:
                    type: integer
                    title: ""
                    format: int32
                  completion_tokens:
                    type: integer
                    title: ""
                    format: int32
                  total_tokens:
                    type: integer
                    title: ""
                    format: int32
          examples:
            application/json:
              id: chatcmpl-123
              object: chat.completion
              created: 1677652288
              choices:
                - index: 0
                  message:
                    role: assistant
                    content: Hello there, how may I assist you today?
                  finish_reason: stop
              usage:
                prompt_tokens: 9
                completion_tokens: 12
                total_tokens: 21
      x-ms-openai-data:
        openai-enabled: false
        operations:
          - operationId: Chat
            x-ms-require-user-confirmation: true
definitions:
  errorResponse:
    type: object
    properties:
      error:
        type: object
        properties:
          code:
            type: string
          message:
            type: string
          param:
            type: string
          type:
            type: string
tags: []

